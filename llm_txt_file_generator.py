# -*- coding: utf-8 -*-
"""llm.txt file generator

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZYPhH3GRjgJoVhQVJzxWt9JmGbpOroB
"""

import streamlit as st
import requests
from bs4 import BeautifulSoup
import datetime

COMMON_SITEMAP_PATHS = [
    "sitemap.xml",
    "sitemap_index.xml",
    "sitemap1.xml",
    "blog-sitemap.xml",
    "page-sitemap.xml"
]

def fetch_title_desc(url):
    """Fetch page title and meta description"""
    try:
        response = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        title = soup.title.string.strip() if soup.title else url
        desc_tag = soup.find("meta", attrs={"name": "description"})
        desc = desc_tag["content"].strip() if desc_tag else "No description available."
        return title, desc
    except Exception:
        return url, "No description available."

def fetch_intro_text(url):
    """Fetch main paragraphs from homepage or About Us page"""
    try:
        response = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        paragraphs = soup.find_all("p")
        text = ""
        for p in paragraphs:
            p_text = p.get_text().strip()
            if len(p_text) > 50:
                text += p_text + "\n\n"
            if len(text.split()) > 150:
                break
        return text.strip() if text else "No introduction available."
    except Exception:
        return "No introduction available."

def get_urls_from_sitemap(sitemap_url):
    """Fetch URLs from sitemap or sitemap index"""
    try:
        r = requests.get(sitemap_url, timeout=10, headers={"User-Agent": "Mozilla/5.0"})
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "xml")
        urls = []

        if soup.find("urlset"):
            urls = [loc.text for loc in soup.find_all("loc")]
        elif soup.find("sitemapindex"):
            sitemap_links = [loc.text for loc in soup.find_all("loc")]
            for sm in sitemap_links:
                try:
                    urls.extend(get_urls_from_sitemap(sm))
                except Exception:
                    continue
        return urls
    except Exception:
        return []

def find_sitemap(domain):
    for path in COMMON_SITEMAP_PATHS:
        url = f"https://{domain}/{path}"
        urls = get_urls_from_sitemap(url)
        if urls:
            return urls
    return []

def generate_llms(domain):
    llms_content = []

    llms_content.append(f"# {domain.replace('www.', '').capitalize()}\n")
    intro_text = fetch_intro_text(f"https://{domain}")
    llms_content.append(intro_text + "\n")

    urls = find_sitemap(domain)
    if not urls:
        try:
            response = requests.get(f"https://{domain}", timeout=10, headers={"User-Agent": "Mozilla/5.0"})
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")
            homepage_links = {a["href"] for a in soup.find_all("a", href=True) if a["href"].startswith("http")}
            urls = list(homepage_links)
        except Exception:
            urls = []

    page_urls = [u for u in urls if "/blog/" not in u and "/post/" not in u]
    blog_urls = [u for u in urls if "/blog/" in u or "/post/" in u]

    llms_content.append("\n## Page\n")
    if page_urls:
        for url in page_urls[:10]:
            title, desc = fetch_title_desc(url)
            llms_content.append(f"- [{title}]({url}): {desc}")
    else:
        llms_content.append("- No pages found.\n")

    llms_content.append("\n## Blog\n")
    if blog_urls:
        for url in blog_urls[:10]:
            title, desc = fetch_title_desc(url)
            llms_content.append(f"- [{title}]({url}): {desc}")
    else:
        llms_content.append("- No blogs found.\n")

    llms_content.append(f"\n_Last updated: {datetime.date.today()}_\n")

    return "\n".join(llms_content)

st.title("LLMs.txt Generator for SEO üåê")
domain_input = st.text_input("Enter Website Domain (e.g., paytm.com):")

if st.button("Generate LLMs.txt"):
    if domain_input:
        with st.spinner("Generating..."):
            output_text = generate_llms(domain_input.strip())
            st.text_area("Generated LLMs Content", value=output_text, height=400)

            # Save file
            with open("llms.txt", "w", encoding="utf-8") as f:
                f.write(output_text)
            st.success("‚úÖ LLMs.txt generated and saved successfully!")
    else:
        st.error("Please enter a valid domain.")

